{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "#Generate sample data\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 45\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "n_clusters = len(centers)\n",
    "X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n",
    "# Compute clustering with Means\n",
    "k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "t0 = time.time()\n",
    "k_means.fit(X)\n",
    "t_batch = time.time() - t0\n",
    "# Compute clustering with MiniBatchKMeans\n",
    "\n",
    "# Plot result\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\n",
    "colors = ['#4EACC5', '#FF9C34', '#4E9A06']\n",
    "\n",
    "# We want to have the same colors for the same cluster from the\n",
    "# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per\n",
    "# closest one.\n",
    "k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=0)\n",
    "#k_means_cluster_centers = np.load('E:/PyDevWorkSpaceTest/Ensembles/Chapter_01/data/kmenasCenter.npy')\n",
    "# np.save('E:/PyDevWorkSpaceTest/Ensembles/Chapter_01/data/kmenasCenter.npy',k_means_cluster_centers)\n",
    "k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2,1)\n",
    "# ax.plot(X[:, 0], X[:, 1], 'w',markerfacecolor='k', marker='.',markersize=8)\n",
    "# KMeans\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = k_means_labels == k\n",
    "    cluster_center = k_means_cluster_centers[k]\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w',markerfacecolor=col, marker='.',markersize=8)\n",
    "#     plt.text(X[my_members, 0], X[my_members, 1],  '%i' % (k))\n",
    "    ax.plot(cluster_center[0], cluster_center[1], marker='o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=10)\n",
    "    plt.text(cluster_center[0], cluster_center[1],  'Cluster: %i' % (k))\n",
    "\n",
    "# ax.set_title('KMeans')\n",
    "\n",
    "\n",
    "test_point = [-1.3,1.3]\n",
    "ax.plot(test_point[0],test_point[1],marker='x',markerfacecolor='r',markersize=12)\n",
    "#plt.text(test_point[0],test_point[1],  'point:%.1f,%.1f' % (test_point[0],test_point[1]))\n",
    "#Check out its distance from each of the cluster\n",
    "dist = []\n",
    "for center in k_means_cluster_centers:\n",
    "    dist.append((sum(np.square((center) - (test_point)))))\n",
    "\n",
    "min = np.argmin(dist)\n",
    "test_point = [-1.3,1.3]\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = k_means_labels == k\n",
    "    cluster_center = k_means_cluster_centers[k]\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w',markerfacecolor=col, marker='.',markersize=8)\n",
    "#     plt.text(X[my_members, 0], X[my_members, 1],  '%i' % (k))\n",
    "    ax.plot(cluster_center[0], cluster_center[1], marker='o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=10)\n",
    "    plt.text(cluster_center[0], cluster_center[1],  'Cluster: %i' % (k))\n",
    "ax.plot(test_point[0],test_point[1],marker='x',markerfacecolor='r',markersize=8)\n",
    "plt.text(test_point[0],test_point[1],  '%i' % (min))\n",
    "\n",
    "print('distances are: '+ str(dist))\n",
    "print('Minimum distance index: '+str(min))        \n",
    "\n",
    "\n",
    "#Supervised algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.metrics import log_loss\n",
    "y = k_means_labels\n",
    "\n",
    "X_train, y_train = X[:2000], y[:2000]\n",
    "X_valid, y_valid = X[2000:2500], y[2000:2500]\n",
    "X_train_valid, y_train_valid = X[:2500], y[:2500]\n",
    "X_test, y_test = X[2500:], y[2500:]\n",
    "\n",
    "# Train uncalibrated random forest classifier on whole train and validation\n",
    "# data and evaluate on test data\n",
    "clf = rf(n_estimators=25)\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "clf_probs = clf.predict_proba(X_test)\n",
    "\n",
    "pred_label = np.argmax(clf_probs,axis=1)\n",
    "# score = log_loss(y_test, clf_probs)\n",
    "nnz = np.shape(y_test)[0] - np.count_nonzero(pred_label - y_test)\n",
    "acc = 100*nnz/np.shape(y_test)[0]\n",
    "print('accuracy is: '+str(acc))\n",
    "\n",
    "clf_probs = clf.predict_proba(test_point)\n",
    "pred_label = np.argmax(clf_probs,axis=1)\n",
    "print('RF predicted label: '+str(pred_label))\n",
    "plt.show()\n",
    "# ax.set_xticks(())\n",
    "# ax.set_yticks(())\n",
    "# plt.text(-3.5, 1.8,  'train time: %.2fs\\ninertia: %f' % (\n",
    "#     t_batch, k_means.inertia_))\n",
    "\n",
    "# MiniBatchKMeans\n",
    "# ax = fig.add_subplot(1, 3, 2)\n",
    "# for k, col in zip(range(n_clusters), colors):\n",
    "#     my_members = mbk_means_labels == order[k]\n",
    "#     cluster_center = mbk_means_cluster_centers[order[k]]\n",
    "#     ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n",
    "#             markerfacecolor=col, marker='.')\n",
    "#     ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "#             markeredgecolor='k', markersize=6)\n",
    "# ax.set_title('MiniBatchKMeans')\n",
    "# ax.set_xticks(())\n",
    "# ax.set_yticks(())\n",
    "# # plt.text(-3.5, 1.8, 'train time: %.2fs\\ninertia: %f' %\n",
    "# #          (t_mini_batch, mbk.inertia_))\n",
    "# \n",
    "# # Initialise the different array to all False\n",
    "# different = (mbk_means_labels == 4)\n",
    "# ax = fig.add_subplot(1, 3, 3)\n",
    "# \n",
    "# for k in range(n_clusters):\n",
    "#     different += ((k_means_labels == k) != (mbk_means_labels == order[k]))\n",
    "# \n",
    "# identic = np.logical_not(different)\n",
    "# ax.plot(X[identic, 0], X[identic, 1], 'w',\n",
    "#         markerfacecolor='#bbbbbb', marker='.')\n",
    "# ax.plot(X[different, 0], X[different, 1], 'w',\n",
    "#         markerfacecolor='m', marker='.')\n",
    "# ax.set_title('Difference')\n",
    "# ax.set_xticks(())\n",
    "# ax.set_yticks(())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
